# TODO: Automatyzacja procesu aktualizacji danych ESMA

## üéØ Cel

Zamkniƒôcie ca≈Çej logiki sprawdzania strony WWW ESMA, pobierania pliku CSV, przeprowadzenia walidacji, czyszczenia i importu do bazy danych w jeden zautomatyzowany proces, kt√≥ry bƒôdzie mo≈ºna uruchomiƒá jako cron job.

## üìã Zakres zadania

### 1. Sprawdzanie strony ESMA i pobieranie pliku

- [ ] Implementacja sprawdzania strony ESMA pod kƒÖtem nowych aktualizacji
- [ ] Automatyczne wykrywanie nowego pliku CSV (por√≥wnanie dat publikacji)
- [ ] Pobieranie najnowszego pliku CSV z ESMA
- [ ] Zapisywanie pliku w `data/raw/` z odpowiedniƒÖ nazwƒÖ (format: `CASPYYYYMMDD.csv`)

### 2. Pe≈Çny pipeline przetwarzania

- [ ] Walidacja surowego pliku CSV ‚Üí `reports/validation/raw/`
- [ ] Deterministic cleaning ‚Üí `data/cleaned/` + `reports/cleaning/`
- [ ] Walidacja wyczyszczonego pliku ‚Üí `reports/validation/clean/`
- [ ] (Opcjonalnie) LLM Remediation dla pozosta≈Çych b≈Çƒôd√≥w:
  - Generowanie zada≈Ñ remediacji ‚Üí `reports/remediation/tasks/`
  - Uruchomienie LLM remediation ‚Üí `reports/remediation/patches/`
  - Zastosowanie patch'a ‚Üí `data/cleaned/` + `reports/remediation/apply/`
- [ ] Finalna walidacja ‚Üí `reports/validation/final/` (MUSI przej≈õƒá)
- [ ] Import do bazy danych (lokalnie lub przez API endpoint)

### 3. Skrypt automatyzacji

- [x] Utworzenie g≈Ç√≥wnego skryptu `scripts/update_esma_data.py` (lub `.sh`)
- [x] Integracja wszystkich krok√≥w pipeline'u (sprawdzanie ESMA, pobieranie, walidacja, cleaning, LLM)
- [x] Obs≈Çuga b≈Çƒôd√≥w i logowanie
- [ ] Automatyczny commit i push do GitHub
- [ ] Automatyczne wywo≈Çanie Railway API importu
- [ ] Notyfikacje (email/Slack/webhook) z raportem
- [ ] Konfiguracja przez plik `.env` lub `config.yaml`

### 4. Cron Job Setup

- [ ] Konfiguracja dla Linux/macOS (crontab)
- [ ] Konfiguracja dla Railway (Railway Cron Jobs)
- [ ] Konfiguracja dla GitHub Actions (`.github/workflows/update_esma_data.yml`)
- [ ] Dokumentacja konfiguracji cron job

### 5. Testy i dokumentacja

- [ ] Testy jednostkowe dla poszczeg√≥lnych krok√≥w
- [ ] Testy integracyjne dla pe≈Çnego pipeline'u
- [ ] Dokumentacja w `UPDATE_DATA.md`
- [ ] Przyk≈Çady konfiguracji cron job

## üîß Wymagania techniczne

### Zale≈ºno≈õci

- Python 3.11+
- Biblioteki do web scraping (np. `requests`, `beautifulsoup4`)
- Wszystkie istniejƒÖce zale≈ºno≈õci z `backend/requirements.txt`
- `python-dotenv` dla konfiguracji

### Konfiguracja

Plik `.env` powinien zawieraƒá:
```env
# Database
DATABASE_URL=postgresql://user:password@host:port/database

# LLM Remediation (opcjonalne)
GEMINI_API_KEY=your_api_key_here

# ESMA Configuration
ESMA_REGISTER_URL=https://www.esma.europa.eu/...
ESMA_CSV_DOWNLOAD_URL=...

# Notifications (opcjonalne)
SLACK_WEBHOOK_URL=...
EMAIL_NOTIFICATION=admin@example.com

# Cron Job Configuration
CRON_SCHEDULE=0 2 * * *  # Daily at 2 AM
TIMEZONE=Europe/Warsaw
```

### Struktura skryptu

```python
# scripts/update_esma_data.py
def main():
    # 1. Check ESMA for updates
    # 2. Download CSV if new version available
    # 3. Validate raw CSV
    # 4. Clean CSV
    # 5. Validate cleaned CSV
    # 6. (Optional) LLM Remediation
    # 7. Final validation
    # 8. Import to database
    # 9. Send notifications
    # 10. Log results
```

## üìù Deliverables

1. **Skrypt automatyzacji**: `scripts/update_esma_data.py`
2. **Konfiguracja cron job**: `.github/workflows/update_esma_data.yml`
3. **Dokumentacja**: Zaktualizowany `UPDATE_DATA.md` z sekcjƒÖ o automatyzacji
4. **Testy**: Testy dla pe≈Çnego pipeline'u
5. **Konfiguracja**: Przyk≈Çady konfiguracji dla r√≥≈ºnych ≈õrodowisk

## üéØ Success Criteria

- ‚úÖ Skrypt automatycznie sprawdza ESMA i pobiera nowy plik
- ‚úÖ Pe≈Çny pipeline (walidacja ‚Üí cleaning ‚Üí LLM) dzia≈Ça automatycznie
- ‚è≥ Automatyczny commit/push i Railway import (w trakcie)
- ‚è≥ Cron job uruchamia siƒô zgodnie z harmonogramem (planowane)
- ‚è≥ Notyfikacje sƒÖ wysy≈Çane po ka≈ºdej aktualizacji (planowane)
- ‚úÖ Wszystkie b≈Çƒôdy sƒÖ logowane i raportowane
- ‚úÖ Pipeline ko≈Ñczy siƒô sukcesem lub bezpiecznie przerywa w przypadku b≈Çƒôd√≥w

## ‚è±Ô∏è Szacowany czas

- **Sprawdzanie ESMA i pobieranie**: 4-6 godzin
- **Integracja pipeline'u**: 3-4 godziny
- **Cron job setup**: 2-3 godziny
- **Testy i dokumentacja**: 3-4 godziny

**Total**: ~12-17 godzin

## üîó PowiƒÖzane pliki

- `scripts/validate_csv.py` - walidacja CSV
- `scripts/clean_csv.py` - czyszczenie CSV
- `scripts/generate_remediation_tasks.py` - generowanie zada≈Ñ LLM
- `scripts/run_llm_remediation.py` - uruchomienie LLM remediation
- `scripts/apply_remediation_patch.py` - zastosowanie patch'a
- `backend/app/routers/entities.py` - endpoint `/api/admin/import`
- `UPDATE_DATA.md` - dokumentacja procesu aktualizacji

---

**Status**: üü¢ Partially Implemented (orchestration script ready, full automation pending)  
**Last Updated**: 2025-12-30  
**Priority**: High
