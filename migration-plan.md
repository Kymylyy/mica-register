# EKSPRESOWY Deployment Plan - 2026-02-01 (15:50-17:00)

**UWAGA:** To jest skr√≥cona wersja oryginalnego planu, dostosowana do rozpoczƒôcia deployment o 15:20 zamiast 08:30.

**Data:** 2026-02-01 (dzisiaj, start: 15:50)
**Typ:** Major architectural refactor (system multi-register)
**Platformy:** Railway (backend) + Vercel (frontend)
**Czas:** 15:50-17:00 (1h 10min - NAPIƒòTY HARMONOGRAM)

---

## üìä G≈Ç√≥wne Wnioski

**Dobra wiadomo≈õƒá:** 50% przygotowa≈Ñ ju≈º zrobione!
- ‚úÖ Migration 002 poprawiona (dti_ffg ‚Üí VARCHAR)
- ‚úÖ backend/start.sh istnieje (z error handling)
- ‚úÖ API migration Gemini ‚Üí Deepseek COMPLETED
- ‚úÖ CSV files ju≈º w repo (15 plik√≥w tracked)
- ‚úÖ Dockerfile zaktualizowany (u≈ºywa start.sh)
- ‚úÖ models.py, schemas.py, import_csv.py - wszystkie poprawki zrobione
- ‚úÖ Frontend - 5 register tabs gotowe
- ‚úÖ Testy - wszystkie 33 fixed

**Problem:** Oryginalny plan nieaktualny w kluczowych aspektach:
- ‚ö†Ô∏è Timeline zak≈Çada start o 08:30, ale jest **15:50** (zosta≈Ço tylko 1h 10min!)
- ‚ö†Ô∏è Wszƒôdzie GEMINI_API_KEY zamiast DEEPSEEK_API_KEY
- ‚ö†Ô∏è Zak≈Çada dodawanie CSV files, kt√≥re sƒÖ **ju≈º w repo**
- ‚ö†Ô∏è Zak≈Çada tworzenie start.sh, kt√≥ry **ju≈º istnieje**
- ‚ö†Ô∏è Plan zak≈Çada pe≈Çne testy (brak czasu)

**Stan deployment:**
- ‚ùå NIE rozpoczƒôty - llms-control NIE zmergowany do main
- ‚ùå Kod nie zacommitowany (11 plik√≥w + start.sh untracked)
- ‚úÖ Wiƒôkszo≈õƒá zmian kodu gotowa - tylko commit + merge + deploy

**Realno≈õƒá ekspresowego deployment:**
- ‚è±Ô∏è Mo≈ºliwe w 1h 40min (napiƒôty ale wykonalny harmonogram)
- ‚ö†Ô∏è Pominiƒôte: backup DB, pe≈Çne testy E2E (zrobiƒá jutro)
- ‚úÖ Kluczowe: weryfikacja env vars PRZED merge

---

## ‚ö†Ô∏è UWAGA - Skr√≥cony Plan

**Status przed startem (15:50):**
- ‚úÖ 50% przygotowa≈Ñ ju≈º zrobione
- ‚úÖ CSV files w repo (15 plik√≥w)
- ‚úÖ Migration 002 poprawiona (dti_ffg VARCHAR)
- ‚úÖ backend/start.sh istnieje
- ‚úÖ Dockerfile zaktualizowany
- ‚úÖ API migration: Gemini ‚Üí Deepseek COMPLETED
- ‚ùå Kod nie zacommitowany (11 plik√≥w + start.sh)
- ‚ùå Nie zmergowano do main

**Pominiƒôte (brak czasu):**
- ‚è≠Ô∏è Backup bazy danych (ryzyko - zr√≥b jutro!)
- ‚è≠Ô∏è Pe≈Çne testy lokalne (tylko quick smoke test)
- ‚è≠Ô∏è Full E2E testing (zr√≥b jutro post-deployment)

---

## Podsumowanie Zmian (Status: READY)

### ‚úÖ Co Wdra≈ºamy
- **G≈Ç√≥wna zmiana:** System jednego rejestru (CASP) ‚Üí Architektura multi-register (5 rejestr√≥w)
- **Zakres:** 102 pliki, +17,965 linii kodu
- **Baza danych:** Destrukcyjna migracja schematu (migration 002 - JU≈ª POPRAWIONA)
- **Frontend:** Dodana zale≈ºno≈õƒá `react-router-dom` v6.30.3
- **Environment:** `DEEPSEEK_API_KEY` opcjonalny (tylko dla remediation features)

### ‚úÖ KRYTYCZNE ZMIANY - Status

**‚úÖ JU≈ª ZROBIONE (do zacommitowania):**
- ‚úÖ Migration 002: dti_ffg zmienione z BOOLEAN ‚Üí VARCHAR (linie 225, 267)
- ‚úÖ backend/start.sh: Auto-migrations script (ISTNIEJE, untracked)
- ‚úÖ Dockerfile: Zaktualizowany do u≈ºywania start.sh (linia 31)
- ‚úÖ models.py: dti_ffg jako String (linie 273, 319)
- ‚úÖ schemas.py: dti_ffg jako Optional[str] (linia 86)
- ‚úÖ import_csv.py: Parsing dti_ffg jako string (linie 484, 530)
- ‚úÖ routers/entities.py: Rozszerzony search (address, website, register-specific)
- ‚úÖ Frontend App.jsx: 5 register tabs
- ‚úÖ CSV files: JU≈ª W REPO (15 plik√≥w)
- ‚úÖ API Migration: Gemini ‚Üí Deepseek COMPLETED

**‚ùå DO ZROBIENIA (15:50-17:00):**
- ‚ùå Commit + push changes
- ‚ùå Create PR
- ‚ùå Merge to main
- ‚ùå Monitor deployment
- ‚ùå Import danych
- ‚ùå Smoke tests

---

## TIMELINE EKSPRESOWY (15:50-17:00)

### 15:50-16:00: GIT WORKFLOW (10 min)

**Commit uncommitted changes:**

```bash
cd /Users/Kymyly/Desktop/GIT/mica-register

# Verify current branch
git branch  # Should show: * llms-control

# Add untracked start.sh
git add backend/start.sh

# Stage all modified files
git add Dockerfile
git add backend/app/models.py
git add backend/app/schemas.py
git add backend/app/import_csv.py
git add backend/app/routers/entities.py
git add backend/migrations/002_multi_register_migration.py
git add frontend/src/App.jsx
git add frontend/src/components/DataTable.jsx
git add frontend/src/config/registerColumns.js
git add update_production.sh
git rm IMPLEMENTATION_SUMMARY.md  # Already deleted

# Verify staged files
git status

# Commit
git commit -m "$(cat <<'EOF'
Deploy: Multi-register architecture ready

Critical changes:
- Migration 002: dti_ffg BOOLEAN ‚Üí VARCHAR (lines 225, 267)
- Startup script: Auto-migrations for Railway (backend/start.sh)
- Dockerfile: Uses startup script for auto-migrations
- API migration: Gemini ‚Üí Deepseek COMPLETED
  - llm_client.py: DEEPSEEK_API_KEY environment variable
  - Model fallback: deepseek-reasoner ‚Üí deepseek-chat
- Models/schemas: dti_ffg as String/Optional[str]
- Import CSV: dti_ffg parsed as string identifier
- Frontend: Multi-register tabs (CASP, OTHER, ART, EMT, NCASP)
- Routers: Enhanced search (address, website, register-specific fields)
- Tests: All 33 tests fixed

Ready for production deployment.

Co-Authored-By: Claude Sonnet 4.5 <noreply@anthropic.com>
EOF
)"

# Push to origin
git push origin llms-control
```

**‚úÖ Checkpoint:** Verify push succeeded (git log, GitHub)

---

### 16:00-16:05: VERIFY .dockerignore (5 min)

‚ö†Ô∏è **KRYTYCZNE:** Upewnij siƒô ≈ºe CSV files nie sƒÖ ignorowane w Docker build

```bash
# Check if .dockerignore exists
cat .dockerignore 2>/dev/null || echo "No .dockerignore (OK - all files copied)"

# If .dockerignore exists, verify it doesn't exclude data/
grep -E "^data/|^\*\.csv" .dockerignore && echo "‚ö†Ô∏è WARNING: .dockerignore blocks CSV files!" || echo "‚úÖ OK"

# If blocked: Remove data/ from .dockerignore or add exception:
# !data/cleaned/**
```

**Railway needs CSV files w kontenerze** - bez tego import failuje z "CSV not found"

### 16:05-16:10: QUICK LOCAL TEST (5 min - SKR√ìCONY)

```bash
cd /Users/Kymyly/Desktop/GIT/mica-register

# Fresh SQLite DB
rm -f backend/database.db

# Test startup script (migrations + uvicorn)
cd backend
bash start.sh &
BACKEND_PID=$!

# Wait for startup
sleep 5

# Quick smoke tests
curl http://localhost:8000/
# Expected: {"message": "MiCA Register API"} or similar

curl http://localhost:8000/api/entities?register_type=casp | jq '. | length'
# Expected: 0 (empty - no data imported yet)

curl http://localhost:8000/api/services | jq '. | length'
# Expected: 10 (services a-j)

# Check logs for migrations
# Should see: "üîß Running database migrations..." "‚úÖ All migrations completed successfully"

# Kill backend
kill $BACKEND_PID
cd ..
```

**‚úÖ Checkpoint:** No errors, migrations ran successfully

**‚è≠Ô∏è SKIP (brak czasu):**
- Frontend test
- Full import test
- E2E tests

---

### 16:10-16:18: CREATE PR + VERIFY ENV VARS (8 min)

**‚ö†Ô∏è KRYTYCZNE: Verify environment variables PRZED merge!**

**Vercel Dashboard:**
1. Vercel ‚Üí Project ‚Üí Settings ‚Üí Environment Variables
2. **WYMAGANE:** `VITE_API_URL` = `https://your-app.railway.app` (BEZ trailing slash)
3. Je≈õli nie ustawione: **USTAW TERAZ** (przed merge!)

**Railway Dashboard:**
1. Railway ‚Üí Project ‚Üí Backend Service ‚Üí Variables
2. **WYMAGANE:** `CORS_ORIGINS` = `https://your-app.vercel.app` (bez spacji)
3. **OPCJONALNE:** `DEEPSEEK_API_KEY` (nie GEMINI_API_KEY!) - backend startuje bez tego
   - Pobierz z: https://platform.deepseek.com/api_keys

**Create PR (via GitHub CLI - fastest):**

```bash
gh pr create \
  --title "Deploy: Multi-register architecture (5 ESMA MiCA registers)" \
  --body "$(cat <<'EOF'
## EKSPRESOWY DEPLOYMENT (15:20-17:00)

### Summary
Refactor from single-register (CASP) to multi-register architecture for all 5 ESMA MiCA registers.

### Critical Changes
- **Database:** Multi-register schema (base Entity + 5 extension tables)
- **Migration 002:** dti_ffg BOOLEAN ‚Üí VARCHAR (FIXED)
- **API Migration:** Gemini ‚Üí Deepseek (COMPLETED)
- **Backend:** register_type routing, enhanced search (address, website, register-specific)
- **Frontend:** Tab navigation (5 registers), register-specific columns
- **Dependencies:** react-router-dom v6.30.3

### Auto-Deployment Features
- **Railway:** Auto-migrations via backend/start.sh (runs migration 001 + 002 on startup)
- **Vercel:** SPA routing configured

### Environment Variables Status
‚úÖ VERIFIED BEFORE MERGE:
- Railway: `DATABASE_URL` (auto-set by PostgreSQL service)
- Railway: `CORS_ORIGINS` (set to Vercel URL)
- Railway: `DEEPSEEK_API_KEY` (OPTIONAL - not GEMINI_API_KEY!)
- Vercel: `VITE_API_URL` (set to Railway backend URL)

### Testing Status
‚úÖ Local migrations tested (SQLite)
‚úÖ Startup script tested
‚è≠Ô∏è Full E2E tests (post-deployment tomorrow)

### Breaking Changes
- LEI uniqueness removed (now scoped to register_type)
- API requires register_type parameter (defaults to CASP for compatibility)

### Success Criteria
- All 5 registers accessible via API
- Frontend shows 5 tabs
- Data import successful (~984 entities total; ART mo≈ºe byƒá 0 je≈õli CSV puste)
- No console/API errors

Deployment ready for immediate merge.
EOF
)" \
  --base main \
  --head llms-control
```

**Lub via GitHub UI:**
1. GitHub ‚Üí "Pull Requests" ‚Üí "New Pull Request"
2. Base: `main`, Compare: `llms-control`
3. Copy title + body z powy≈ºszego

**‚úÖ Checkpoint:** PR created, link saved

---

### 16:18-16:20: MERGE TO MAIN (2 min)

**‚è∞ TIMING: Merguj tylko je≈õli env vars zweryfikowane!**

**Via GitHub CLI (fastest):**
```bash
gh pr merge --merge --auto
```

**Lub via GitHub UI:**
- Click "Merge Pull Request"
- Wybierz "Create a merge commit" (nie squash)
- Confirm merge

**To triggeruje:**
- ‚úÖ Railway auto-build (backend) - trwa 5-10 min
- ‚úÖ Vercel auto-build (frontend) - trwa 3-5 min

**‚úÖ Checkpoint:**
- [ ] main branch updated (git log)
- [ ] Railway build started (check dashboard)
- [ ] Vercel build started (check dashboard)

---

### 16:20-16:35: MONITOR BUILDS (15 min)

**Railway Backend Build:**

1. Railway ‚Üí Project ‚Üí Backend Service ‚Üí "Deployments"
2. Obserwuj build logs (5-10 min)

**Oczekiwany output:**
```
Building...
[+] Building 45.3s (12/12) FINISHED
Installing Python dependencies...
‚úì Successfully built
Deploying...

üîß Running database migrations...
Running migration 001: Performance indexes...
‚úÖ Migration 001 completed
Running migration 002: Multi-register schema...
‚úÖ Added register_type column
‚úÖ Created extension tables
‚úÖ Migration 002 completed
‚úÖ All migrations completed successfully

üöÄ Starting FastAPI application...
INFO: Started server process
INFO: Application startup complete
INFO: Uvicorn running on http://0.0.0.0:8000
```

‚ö†Ô∏è **WA≈ªNE - Migracje Idempotentne:**
- Migracje uruchamiajƒÖ siƒô **przy KA≈ªDYM restarcie** kontenera Railway
- Migrations 001 i 002 **MUSZƒÑ byƒá idempotentne** (sprawdzajƒÖ `if not exists` przed tworzeniem)
- Obecne migracje SƒÑ idempotentne - ale je≈õli dodasz nowe, upewnij siƒô ≈ºe te≈º sƒÖ!
- Je≈õli migracja nie jest idempotentna ‚Üí u≈ºyj `railway run python migrations/XXX.py` TYLKO RAZ rƒôcznie

**Je≈õli migrations failujƒÖ:**
```bash
# Fallback: Manual migrations via Railway CLI
railway login
railway link
railway run python migrations/001_add_performance_indexes.py
railway run python migrations/002_multi_register_migration.py
railway restart
```

**Vercel Frontend Build:**

1. Vercel ‚Üí Project ‚Üí "Deployments"
2. Obserwuj build (3-5 min)

**Oczekiwany output:**
```
Running build command: npm run build
‚úì built in 25s
‚úì 125 modules transformed
dist/index.html           0.52 kB
dist/assets/index-abc.js  245.67 kB

Deployment ready
Production: https://your-app.vercel.app
```

**Verify env vars w build logs:**
- Search for "VITE_API_URL" ‚Üí powinno pokazaƒá Railway URL

**‚úÖ Checkpoint:**
- [ ] Railway deployment successful (status: "Active")
- [ ] Vercel deployment successful (status: "Production")
- [ ] Backend logs: "Uvicorn running"
- [ ] Migrations logs: "‚úÖ All migrations completed"

---

### 16:35-17:00: IMPORT DANYCH + VERIFY (25 min)

‚ö†Ô∏è **SECURITY WARNING - Import Endpoint:**
Je≈õli masz endpoint `/api/admin/import-all` w kodzie:
- Jest on **PUBLICZNY** (brak autentykacji!)
- **TODO POST-DEPLOYMENT:** Zabezpieczyƒá przed produkcjƒÖ:
  - Opcja 1: Dodaƒá Basic Auth / secret token w header
  - Opcja 2: UsunƒÖƒá endpoint z produkcji (tylko CLI import)
  - Opcja 3: Whitelisting IP (Railway, admin IP)
- **NA DZI≈ö:** Import robimy via CLI (bezpieczniejsze)

**Weryfikacja ≈ºe CSV sƒÖ dostƒôpne:**

```bash
railway login
railway link

# Check filesystem
railway shell
ls -lh data/cleaned/casp/
ls -lh data/cleaned/other/
ls -lh data/cleaned/art/
ls -lh data/cleaned/emt/
ls -lh data/cleaned/ncasp/
# Should show CSV files
exit
```

**Import wszystkich rejestr√≥w (via Railway CLI - ZALECANE):**

```bash
# Via Railway shell (no timeout)
railway shell
python backend/import_all_registers.py
# ‚Üë Ten skrypt importuje z data/cleaned/**/*.csv do PostgreSQL
# Wait for completion (may take 10-20 min)
exit
```

**Oczekiwany output:**
```
INFO: Importing CASP... 149 entities
INFO: Importing OTHER... 705 entities
INFO: Importing ART... 1 entities
INFO: Importing EMT... 32 entities
INFO: Importing NCASP... 102 entities
Total: 984 entities (ART mo≈ºe byƒá 0 je≈õli CSV puste)
```

**Weryfikacja importu (API calls):**

```bash
# Set backend URL
BACKEND_URL="https://your-app.railway.app"

# Test ka≈ºdego rejestru
curl "$BACKEND_URL/api/entities?register_type=casp" | jq '. | length'    # Expected: ~149
curl "$BACKEND_URL/api/entities?register_type=other" | jq '. | length'   # Expected: ~705
curl "$BACKEND_URL/api/entities?register_type=art" | jq '. | length'     # Expected: ~1
curl "$BACKEND_URL/api/entities?register_type=emt" | jq '. | length'     # Expected: ~32
curl "$BACKEND_URL/api/entities?register_type=ncasp" | jq '. | length'   # Expected: ~102

# Total check
echo "Total entities:"
curl -s "$BACKEND_URL/api/entities?register_type=casp" | jq '. | length' > /tmp/counts.txt
curl -s "$BACKEND_URL/api/entities?register_type=other" | jq '. | length' >> /tmp/counts.txt
curl -s "$BACKEND_URL/api/entities?register_type=art" | jq '. | length' >> /tmp/counts.txt
curl -s "$BACKEND_URL/api/entities?register_type=emt" | jq '. | length' >> /tmp/counts.txt
curl -s "$BACKEND_URL/api/entities?register_type=ncasp" | jq '. | length' >> /tmp/counts.txt
awk '{sum+=$1} END {print sum}' /tmp/counts.txt
# Expected: ~984 (ART mo≈ºe byƒá 0)
```

**‚úÖ Checkpoint:**
- [ ] CASP: ~149 entities
- [ ] OTHER: ~705 entities
- [ ] ART: ~0 entity (je≈õli CSV puste)
- [ ] EMT: ~32 entities
- [ ] NCASP: ~102 entities
- [ ] **Total: ~984 entities** (ART mo≈ºe byƒá 0)

---

### 16:35-17:00: SMOKE TESTS (25 min)

**Frontend Smoke Tests:**

**Odwied≈∫:** `https://your-app.vercel.app`

**Podstawowe testy:**
- [ ] **Strona siƒô ≈Çaduje** (brak bia≈Çego ekranu)
- [ ] **5 zak≈Çadek widocznych:** CASP, OTHER, ART, EMT, NCASP
- [ ] **CASP tab (default):**
  - [ ] ~149 entities wy≈õwietlonych
  - [ ] Filtry widoczne (Home Member State, Services, Date)
  - [ ] Sortowanie dzia≈Ça (kliknij nag≈Ç√≥wek kolumny)
  - [ ] Modal encji otwiera siƒô (kliknij wiersz)
  - [ ] Copy buttons dzia≈ÇajƒÖ (LEI, address)
- [ ] **OTHER tab:**
  - [ ] ~705 entities wy≈õwietlonych
  - [ ] DTI FFG wy≈õwietla siƒô jako STRING (np. "1SL20Z9P1", NIE "Yes/No")
  - [ ] White Paper URL wy≈õwietla siƒô
  - [ ] Filtry dzia≈ÇajƒÖ
- [ ] **Prze≈ÇƒÖczanie zak≈Çadek:**
  - [ ] ART ‚Üí ~0 entity (je≈õli CSV puste)
  - [ ] EMT ‚Üí ~32 entities
  - [ ] NCASP ‚Üí ~102 entities

**Network tab (F12):**
- [ ] **API calls do Railway backend:** `https://your-app.railway.app/api/entities?register_type=...`
- [ ] **Brak b≈Çƒôd√≥w CORS**
- [ ] **Brak 404/500 errors**

**Console tab (F12):**
- [ ] **Brak b≈Çƒôd√≥w JavaScript**
- [ ] **Brak warning "VITE_API_URL not set"**

**Backend API Tests:**

```bash
BACKEND_URL="https://your-app.railway.app"

# Test root endpoint
curl "$BACKEND_URL/"
# Expected: 200 OK

# Test services
curl "$BACKEND_URL/api/services" | jq '. | length'
# Expected: 10

# Test filter counts
curl "$BACKEND_URL/api/filter-counts?register_type=casp" | jq '.home_member_states | length'
# Expected: >= 0

# Test nowego rozszerzonego search (NOWA FUNKCJONALNO≈öƒÜ)
curl "$BACKEND_URL/api/entities?register_type=casp&search=crypto" | jq '. | length'
# Expected: >= 0 (no errors)

# Test search w register-specific fields
curl "$BACKEND_URL/api/entities?register_type=other&search=white" | jq '. | length'
# Expected: >= 0

# Test pagination
curl "$BACKEND_URL/api/entities?register_type=other&page=1&limit=10" | jq '. | length'
# Expected: 10

# Test CORS
curl -I -X OPTIONS "$BACKEND_URL/api/entities" \
  -H "Origin: https://your-app.vercel.app" \
  -H "Access-Control-Request-Method: GET"
# Expected: Access-Control-Allow-Origin header present
```

**‚úÖ Checkpoint:** Wszystkie smoke tests passed

---

## Environment Variables (VERIFY BEFORE MERGE!)

**Railway:**
- `DATABASE_URL` - ‚úÖ Auto-set by Railway PostgreSQL service
- `CORS_ORIGINS` - ‚ö†Ô∏è **KRYTYCZNE:** Ustaw WSZYSTKIE domeny produkcyjne (nie tylko Vercel!)
  ```
  https://micaregister.com,https://www.micaregister.com,https://micaregister.eu,https://www.micaregister.eu,https://mica-register.vercel.app
  ```
  **Dlaczego:** U≈ºytkownicy wchodzƒÖ przez custom domeny - bez tego CORS errors!

- `DEEPSEEK_API_KEY` - OPTIONAL (nie GEMINI_API_KEY! Deepseek migration completed)

**Vercel:**
- `VITE_API_URL` - ‚ö†Ô∏è **KRYTYCZNE:** Ustaw backend URL
  - **Je≈õli masz custom domain** na Railway (np. `api.micaregister.com`): U≈ºyj tego!
  - **Je≈õli nie:** U≈ºyj Railway auto-domain: `https://mica-register-production.up.railway.app`

- ‚ö†Ô∏è **WA≈ªNE - Vercel Environment:**
  - Ustaw w **Production** environment (nie Preview!)
  - Vercel: Settings ‚Üí Environment Variables ‚Üí Environment: **Production** ‚úì
  - Preview builds u≈ºywajƒÖ innych env vars - upewnij siƒô ≈ºe ustawiasz Production

‚ö†Ô∏è **Timing:** Set BEFORE merge, or redeploy after setting!
‚ö†Ô∏è **Vite caveat:** Env vars sƒÖ wbudowane w build time (nie runtime) - ka≈ºda zmiana wymaga redeploy

---

## ROLLBACK PLAN (je≈õli potrzeba)

**Kiedy rollback:**
- Migracja bazy danych failuje (corrupted schema)
- Backend nie startuje (crash loop)
- Krytyczne bugi (data loss, security issue)
- Frontend nie mo≈ºe po≈ÇƒÖczyƒá siƒô z backend (CORS errors)

**Code Rollback:**

```bash
# Znajd≈∫ merge commit
git log --oneline -5

# Revert merge (tworzy nowy commit)
git revert -m 1 <merge-commit-hash>
git push origin main

# Railway i Vercel auto-deploy revert
```

**‚ö†Ô∏è WARNING:** Database rollback mo≈ºe straciƒá zmigrowane dane. Je≈õli deployment failuje przed importem, rollback jest bezpieczny.

---

## KRYTERIA SUKCESU

**Deployment jest sukcesem gdy:**
1. ‚úÖ Backend responds do wszystkich 5 register APIs
2. ‚úÖ Frontend pokazuje wszystkie 5 register tabs
3. ‚úÖ Data loads correctly (~984 entities total; ART mo≈ºe byƒá 0)
4. ‚úÖ Filters, sorting, modals dzia≈ÇajƒÖ
5. ‚úÖ No console errors, no API errors
6. ‚úÖ Rozszerzony search dzia≈Ça (address, website, register-specific)
7. ‚úÖ DTI FFG wy≈õwietla siƒô jako string (nie Yes/No)
8. ‚úÖ No errors w Railway/Vercel logs

---

## KRYTYCZNE ZMIANY vs Oryginalny Plan

**‚úÖ COMPLETED:**
- GEMINI_API_KEY ‚Üí DEEPSEEK_API_KEY migration
- CSV files ju≈º w repo (nie trzeba dodawaƒá)
- backend/start.sh ju≈º istnieje
- Migration 002 ju≈º poprawiona
- dti_ffg ju≈º jako VARCHAR/String

**‚è≠Ô∏è POMINIƒòTE (brak czasu):**
- Backup bazy danych Railway (RYZYKO - zr√≥b jutro!)
- Pe≈Çne testy lokalne (tylko quick smoke test)
- Full E2E testing (zr√≥b jutro post-deployment)
- Tag git (pre-deployment backup)

**‚ö†Ô∏è TIMELINE:**
- Oryginalny: 08:30-17:00 (8.5h)
- Rzeczywisty: 15:20-17:00 (1h 40min)
- Skr√≥cony o: 6h 50min

---

## POST-DEPLOYMENT (jutro 2026-02-02)

**TODO po deployment:**
1. ‚¨ú **KRYTYCZNE - Zabezpiecz import endpoint** (je≈õli istnieje `/api/admin/import-all`):
   - Dodaj Basic Auth lub secret token w header
   - Albo usu≈Ñ endpoint z produkcji (tylko CLI import)
   - Albo whitelist IP addresses (Railway CLI, admin IP)
2. ‚¨ú Full E2E testing (wszystkie edge cases)
3. ‚¨ú **Backup bazy danych Railway** (WA≈ªNE - pominiƒôte dzi≈õ z braku czasu!)
4. ‚¨ú Monitor logs przez 24h (check errors, performance)
5. ‚¨ú Usu≈Ñ stare CSV files z data/cleaned/ (cleanup duplicates)
6. ‚¨ú Update dokumentacji (README.md, deployment guide)
7. ‚¨ú Performance testing (load time, query speed)
8. ‚¨ú Verify wszystkie filtry w production
9. ‚¨ú Test mobile responsive design
10. ‚¨ú Create git tag: `multi-register-deploy-2026-02-01`
11. ‚¨ú Verify .dockerignore (upewnij siƒô ≈ºe data/ nie jest ignorowane)

---

## NOTATKI

**Railway Tips:**
- Build time: 5-10 min (bƒÖd≈∫ cierpliwy)
- Logs sƒÖ kluczowe (monitor non-stop)
- `railway shell` useful dla import (no timeout)

**Vercel Tips:**
- Env vars w build time (nie runtime)
- Po zmianie VITE_API_URL: Redeploy required

**Je≈õli co≈õ p√≥jdzie ≈∫le:**
1. NIE PANIKUJ
2. Check Railway logs (backend errors)
3. Check Vercel logs (build errors)
4. Check browser console (frontend errors)
5. Rollback je≈õli krytyczne

**Powodzenia! üöÄ**

_Plan zaktualizowany z oryginalnego migration-plan.md do ekspresowego deployment (15:20-17:00) z uwagami Codex wdro≈ºonymi._
